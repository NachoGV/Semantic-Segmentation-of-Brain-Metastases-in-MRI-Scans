{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(\"../utils\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Transforms import Transforms\n",
    "from Inference import inference\n",
    "from monai.losses import DiceLoss\n",
    "from Datasets import EnsembleDataset\n",
    "from monai.metrics import DiceMetric\n",
    "from Models import LogisticRegression\n",
    "from monai.transforms import AsDiscrete\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SequentialSampler   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 33\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids = pd.read_csv('../data/TRAIN.csv')['SubjectID'].values\n",
    "\n",
    "ahnet_dice = pd.read_csv('../outputs/AHNet/train_scores.csv')['Dice'].values\n",
    "segresnet_dice = pd.read_csv('../outputs/SegResNet/train_scores.csv')['Dice'].values\n",
    "unet_dice = pd.read_csv('../outputs/UNet/train_scores.csv')['Dice'].values\n",
    "unetr_dice = pd.read_csv('../outputs/UNETR/train_scores.csv')['Dice'].values\n",
    "\n",
    "ah_segs, segresnet_segs, unet_segs, untr_segs, gt_segs = [], [], [], [], []\n",
    "for sid in subject_ids:\n",
    "    ah_channels, unet_channels, segresnet_channels, untr_channels, gt_channels = [], [], [], [], []\n",
    "    for channel in ['TC', 'WT', 'ET']:\n",
    "        ah_channels.append(f'../outputs/AHNet/pred_segs/train_pred_segs/pred_{sid}_{channel}.npz')\n",
    "        segresnet_channels.append(f'../outputs/SegResNet/pred_segs/train_pred_segs/pred_{sid}_{channel}.npz')\n",
    "        unet_channels.append(f'../outputs/UNet/pred_segs/train_pred_segs/pred_{sid}_{channel}.npz')\n",
    "        untr_channels.append(f'../outputs/UNETR/pred_segs/train_pred_segs/pred_{sid}_{channel}.npz')\n",
    "        gt_channels.append(f'../outputs/gt_segs/train_gt_segs/gt_{sid}_{channel}.npz')\n",
    "    \n",
    "    ah_segs.append(ah_channels)\n",
    "    segresnet_segs.append(segresnet_channels)\n",
    "    unet_segs.append(unet_channels)\n",
    "    untr_segs.append(untr_channels)\n",
    "    gt_segs.append(gt_channels)\n",
    "\n",
    "# Dataframe\n",
    "train_df = pd.DataFrame()\n",
    "train_df['SubjectID'] = subject_ids\n",
    "train_df['AHNet'] = ah_segs\n",
    "train_df['UNet'] = unet_segs\n",
    "train_df['SegResNet'] = segresnet_segs\n",
    "train_df['UNETR'] = untr_segs\n",
    "train_df['GT'] = gt_segs\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids = pd.read_csv('../data/VAL.csv')['SubjectID'].values\n",
    "\n",
    "ah_segs, unet_segs, segresnet_segs, untr_segs, gt_segs = [], [], [], [], []\n",
    "for sid in subject_ids:\n",
    "    ah_channels, unet_channels, segresnet_channels, untr_channels, gt_channels = [], [], [], [], []\n",
    "    for channel in ['TC', 'WT', 'ET']:\n",
    "        ah_channels.append(f'../outputs/AHNet/pred_segs/val_pred_segs/pred_{sid}_{channel}.npz')\n",
    "        unet_channels.append(f'../outputs/UNet/pred_segs/val_pred_segs/pred_{sid}_{channel}.npz')\n",
    "        segresnet_channels.append(f'../outputs/SegResNet/pred_segs/val_pred_segs/pred_{sid}_{channel}.npz')\n",
    "        untr_channels.append(f'../outputs/UNETR/pred_segs/val_pred_segs/pred_{sid}_{channel}.npz')\n",
    "        gt_channels.append(f'../outputs/gt_segs/val_gt_segs/gt_{sid}_{channel}.npz')\n",
    "    \n",
    "    ah_segs.append(ah_channels)\n",
    "    unet_segs.append(unet_channels)\n",
    "    segresnet_segs.append(segresnet_channels)\n",
    "    untr_segs.append(untr_channels)\n",
    "    gt_segs.append(gt_channels)\n",
    "\n",
    "# Dataframe\n",
    "val_df = pd.DataFrame()\n",
    "val_df['SubjectID'] = subject_ids\n",
    "val_df['AHNet'] = ah_segs\n",
    "val_df['UNet'] = unet_segs\n",
    "val_df['SegResNet'] = segresnet_segs\n",
    "val_df['UNETR'] = untr_segs\n",
    "val_df['GT'] = gt_segs\n",
    "\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "transforms = Transforms(seed=33)\n",
    "    \n",
    "# Datasets\n",
    "train_dataset = EnsembleDataset(train_df.drop(columns = ['SubjectID']), transform=transforms.train_ensemble((100,100,50)), size = None, include_unet = False)\n",
    "val_dataset = EnsembleDataset(val_df.drop(columns = ['SubjectID']), transform=transforms.val_ensemble(), size = None, include_unet = False)\n",
    "\n",
    "# Samplers\n",
    "train_sampler = SequentialSampler(train_dataset)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = 1, shuffle = False, sampler = train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False, sampler = val_sampler)\n",
    "\n",
    "image, label, og_shape = train_dataset[0]\n",
    "print(image.shape, label.shape, og_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "val_interval = 1\n",
    "threshold=0.9\n",
    "lr=0.001\n",
    "wd=0.0001\n",
    "max_epochs = 10\n",
    "    \n",
    "input_dim = 9\n",
    "output_dim = 3\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "\n",
    "# Post Transforms, Optimizer, Loss, Evaluation Metric\n",
    "trans = AsDiscrete(threshold=threshold)\n",
    "loss_function = DiceLoss(smooth_nr=1e-5, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{max_epochs}\")\n",
    "    print('TRAIN')\n",
    "\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for i, batch in tqdm(enumerate(train_loader)):\n",
    "        # Load\n",
    "        step += 1\n",
    "        images, targets, _ = batch   \n",
    "\n",
    "        # Logistic Regression\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    print(f\"Loss: {epoch_loss}\")\n",
    "\n",
    "    # VAL\n",
    "    if (i+1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        print('---------------------------------------')\n",
    "        print('VAL')\n",
    "        dice_values = []\n",
    "        for batch in tqdm(val_loader):\n",
    "\n",
    "            # Load\n",
    "            images, target, og_shape = batch   \n",
    "\n",
    "            # Predict \n",
    "            outputs = inference(images, 9, model, VAL_AMP=False)\n",
    "            img = trans(outputs).squeeze(0)\n",
    "            target = target.squeeze(0) \n",
    "\n",
    "            # To OG Shape\n",
    "            img = img.mT.reshape(1, og_shape[0], og_shape[1], og_shape[2], og_shape[3])\n",
    "            target = target.mT.reshape(1, og_shape[0], og_shape[1], og_shape[2], og_shape[3])\n",
    "\n",
    "            # Dice Metric\n",
    "            dice_metric(y_pred=img, y=target)\n",
    "            dice_score = dice_metric.aggregate()\n",
    "            dice_values.append(dice_score.item())\n",
    "            dice_metric.reset()\n",
    "\n",
    "        # Results\n",
    "        print('Dice Scores:', np.mean(dice_values))\n",
    "        print('---------------------------------------')\n",
    "\n",
    "    torch.save(model.state_dict(), f'../outputs/EnsembleNU/LogRegCheckpoints/LogisticRegression_{epoch+1}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
